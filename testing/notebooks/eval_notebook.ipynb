{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id_question  id_sample type_question  \\\n",
      "0            1          1       Factual   \n",
      "1            1          1       Factual   \n",
      "2            1          1       Factual   \n",
      "3            1          1       Factual   \n",
      "4            1          1       Factual   \n",
      "\n",
      "                                            question  \\\n",
      "0  Welche Dokumente müssen bei der Beantragung de...   \n",
      "1  Welche Dokumente müssen bei der Beantragung de...   \n",
      "2  Welche Dokumente müssen bei der Beantragung de...   \n",
      "3  Welche Dokumente müssen bei der Beantragung de...   \n",
      "4  Welche Dokumente müssen bei der Beantragung de...   \n",
      "\n",
      "                                     expected_answer  clarity  specificity  \\\n",
      "0  Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
      "1  Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
      "2  Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
      "3  Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
      "4  Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
      "\n",
      "   relevance  clarity_q  specificity_q  ...  \\\n",
      "0          5          4              4  ...   \n",
      "1          5          4              4  ...   \n",
      "2          5          4              4  ...   \n",
      "3          5          4              4  ...   \n",
      "4          5          4              4  ...   \n",
      "\n",
      "                                            keyterms  score  \\\n",
      "0  [\"Dokumente\", \"Beantragung\", \"Klimageschwindig...      5   \n",
      "1  [\"Dokumente\", \"Unterlagen\", \"Antragsunterlagen...      4   \n",
      "2  [\"Unterlagen\", \"Klimageschwindigkeits-Bonus\", ...      4   \n",
      "3  [\"Klimageschwindigkeits-Bonus\", \"Beantragung\",...      5   \n",
      "4  [\"Dokumente\", \"Beantragung\", \"Unterlagen\", \"Kl...      5   \n",
      "\n",
      "                                             comment  type  used_context  \\\n",
      "0  The generated response is fully accurate and a...  Text         False   \n",
      "1  The generated response aligns well with the Ex...  Text         False   \n",
      "2  The generated response accurately lists the tw...  Text         False   \n",
      "3  The response accurately lists the required doc...  Text         False   \n",
      "4  The generated response is fully accurate and a...  Text         False   \n",
      "\n",
      "  retrieved_context used_context_ext retrieved_context_ext total_context_ids  \\\n",
      "0             False            False                 False                34   \n",
      "1             False            False                 False                37   \n",
      "2             False            False                 False                84   \n",
      "3              True            False                  True                33   \n",
      "4              True            False                  True                32   \n",
      "\n",
      "  binary_score  \n",
      "0            1  \n",
      "1            1  \n",
      "2            1  \n",
      "3            1  \n",
      "4            1  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "Index(['id_question', 'id_sample', 'type_question', 'question',\n",
      "       'expected_answer', 'clarity', 'specificity', 'relevance', 'clarity_q',\n",
      "       'specificity_q', 'relevance_q', 'prompt_id', 'device', 'test_name',\n",
      "       'id_test_question', 'doc_type', 'source', 'content', 'id', 'metadata',\n",
      "       'answer', 'begin_date', 'end_date', 'completion_tokens',\n",
      "       'prompt_tokens', 'chunk_size', 'embedding_model', 'alpha_value',\n",
      "       'context_used', 'context_ids', 'context_ids_total', 'alternate_prompts',\n",
      "       'improved_query', 'query_intent', 'keyterms', 'score', 'comment',\n",
      "       'type', 'used_context', 'retrieved_context', 'used_context_ext',\n",
      "       'retrieved_context_ext', 'total_context_ids', 'binary_score'],\n",
      "      dtype='object')\n",
      "(3160, 44)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the root directory of your project\n",
    "project_root = '/Users/rodolfocacacho/Documents/Documents/MAI/Master Thesis/Code/rag_project'\n",
    "os.chdir(project_root)\n",
    "# Add the root directory to sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from config import (CONFIG_SQL_DB,DB_NAME,\n",
    "                    SQL_EVAL_QAS_TABLE_SCHEMA,\n",
    "                    SQL_EVAL_QAS_TABLE, \n",
    "                    EMBEDDING_MODEL,EMBEDDING_MODEL_API,\n",
    "                    EMBEDDING_MODEL_EMB_TASK,\n",
    "                    TEST_RESULTS_TABLE,SQL_EVAL_CHUNKS_TABLE,\n",
    "                    SQL_PROMPTS_TABLE,TEST_GEN_ANSWERS_TABLE,\n",
    "                    RESULTS_DIR)\n",
    "from utils.MySQLDB_manager import MySQLDB\n",
    "from testing.modules.evaluating_modules import RAGEvaluator\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "sql_con = MySQLDB(CONFIG_SQL_DB,DB_NAME)\n",
    "\n",
    "ragEval = RAGEvaluator(sql_con=sql_con,\n",
    "                       test_table_name=TEST_RESULTS_TABLE,\n",
    "                       qas_table_name=SQL_EVAL_QAS_TABLE,\n",
    "                       chunks_eval_table_name=SQL_EVAL_CHUNKS_TABLE,\n",
    "                       prompts_table_name=SQL_PROMPTS_TABLE,\n",
    "                       eval_answers_table=TEST_GEN_ANSWERS_TABLE)\n",
    "\n",
    "df_results = ragEval.data_df\n",
    "\n",
    "print(df_results.head(5))\n",
    "print(df_results.columns)\n",
    "print(df_results.shape)\n",
    "\n",
    "# print(ragEval.generate_report())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_consolidated_test_report(data_df, test_group_cols=['test_name'], groupby_cols=None, output_dir='.',):\n",
    "    \"\"\"\n",
    "    Generate a consolidated test report with all tests in a single sheet for each group.\n",
    "\n",
    "    Args:\n",
    "        data_df (pd.DataFrame): The dataframe containing test data with precomputed metrics.\n",
    "        test_group_cols (list of str): Columns that define the main grouping, e.g., 'test_name'.\n",
    "        groupby_cols (list of str): Additional columns to group by for metrics.\n",
    "        output_file (str): Path to the output Excel file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get the current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Format the date and time\n",
    "    formatted_date = now.strftime(\"%d%m%y%H%M\")\n",
    "\n",
    "    filename = f\"consolidated_test_report_{formatted_date}.xlsx\"\n",
    "    output_file=os.path.join(output_dir,filename)\n",
    "\n",
    "    if groupby_cols is None:\n",
    "        groupby_cols = []\n",
    "\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        # General Summary Sheet\n",
    "        summary_stats = []\n",
    "        for group_values, test_group in data_df.groupby(test_group_cols):\n",
    "            # Ensure group_values is iterable (single or multiple columns)\n",
    "            group_values = [group_values] if not isinstance(group_values, tuple) else list(group_values)\n",
    "            summary_entry = {col: val for col, val in zip(test_group_cols, group_values)}\n",
    "            summary_entry.update({\n",
    "                'Total Questions': len(test_group),\n",
    "                'Avg Prompt Tokens': test_group['prompt_tokens'].mean(),\n",
    "                'Std Prompt Tokens': test_group['prompt_tokens'].std(),\n",
    "                'Avg Answer Tokens': test_group['completion_tokens'].mean(),\n",
    "                'Std Answer Tokens': test_group['completion_tokens'].std(),\n",
    "                'Retrieval Accuracy (used)': test_group['used_context'].mean(),\n",
    "                'Retrieval Accuracy (retrieved)': test_group['retrieved_context'].mean(),\n",
    "                'Expanded Retrieval Accuracy (used)': test_group['used_context_ext'].mean(),\n",
    "                'Expanded Retrieval Accuracy (retrieved)': test_group['retrieved_context_ext'].mean(),\n",
    "                'Avg Score': test_group['score'].mean(),\n",
    "                'Binary Score': test_group['binary_score'].mean(),\n",
    "                'ASS Precision': test_group['precision'].mean(),\n",
    "                'ASS Recall': test_group['recall'].mean(),\n",
    "                'ASS F1': test_group['f1'].mean()\n",
    "            })\n",
    "            summary_stats.append(summary_entry)\n",
    "\n",
    "        summary_df = pd.DataFrame(summary_stats)\n",
    "        summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "\n",
    "        # Group Sheets\n",
    "        for col in groupby_cols:\n",
    "            group_stats = []\n",
    "            for group_values, test_group in data_df.groupby(test_group_cols):\n",
    "                group_values = [group_values] if not isinstance(group_values, tuple) else list(group_values)\n",
    "                for sub_group, group_data in test_group.groupby(col):\n",
    "                    group_entry = {col_name: val for col_name, val in zip(test_group_cols, group_values)}\n",
    "                    group_entry[col] = sub_group\n",
    "                    group_entry.update({\n",
    "                        'Total Questions': len(group_data),\n",
    "                        'Avg Prompt Tokens': group_data['prompt_tokens'].mean(),\n",
    "                        'Std Prompt Tokens': group_data['prompt_tokens'].std(),\n",
    "                        'Avg Answer Tokens': group_data['completion_tokens'].mean(),\n",
    "                        'Std Answer Tokens': group_data['completion_tokens'].std(),\n",
    "                        'Retrieval Accuracy (used)': group_data['used_context'].mean(),\n",
    "                        'Retrieval Accuracy (retrieved)': group_data['retrieved_context'].mean(),\n",
    "                        'Expanded Retrieval Accuracy (used)': group_data['used_context_ext'].mean(),\n",
    "                        'Expanded Retrieval Accuracy (retrieved)': group_data['retrieved_context_ext'].mean(),\n",
    "                        'Avg Score': group_data['score'].mean(),\n",
    "                        'Binary Score': group_data['binary_score'].mean(),\n",
    "                        'ASS Precision': group_data['precision'].mean(),\n",
    "                        'ASS Recall': group_data['recall'].mean(),\n",
    "                        'ASS F1': group_data['f1'].mean()\n",
    "                    })\n",
    "                    group_stats.append(group_entry)\n",
    "\n",
    "            # Convert to DataFrame and write to a single sheet per group\n",
    "            group_stats_df = pd.DataFrame(group_stats)\n",
    "            sheet_name = col[:31]  # Ensure Excel sheet name limit\n",
    "            group_stats_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Consolidated report saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated report saved to results/consolidated_test_report_0712240101.xlsx\n"
     ]
    }
   ],
   "source": [
    "main_test_cols = ['test_name','embedding_model','chunk_size','alpha_value']\n",
    "test_cols = ['type_question','type','doc_type','query_intent']\n",
    "\n",
    "generate_consolidated_test_report(df_results,test_group_cols=main_test_cols,groupby_cols=test_cols,output_dir=RESULTS_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_unstructured",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
