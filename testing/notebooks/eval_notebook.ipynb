{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id_question  id_sample type_question  \\\n",
      "0            1          1       Factual   \n",
      "1            1          1       Factual   \n",
      "2            1          1       Factual   \n",
      "3            1          1       Factual   \n",
      "4            1          1       Factual   \n",
      "\n",
      "                                            question  \\\n",
      "0  Welche Dokumente müssen bei der Beantragung de...   \n",
      "1  Welche Dokumente müssen bei der Beantragung de...   \n",
      "2  Welche Dokumente müssen bei der Beantragung de...   \n",
      "3  Welche Dokumente müssen bei der Beantragung de...   \n",
      "4  Welche Dokumente müssen bei der Beantragung de...   \n",
      "\n",
      "                                     expected_answer  clarity  specificity  \\\n",
      "0  Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
      "1  Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
      "2  Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
      "3  Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
      "4  Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
      "\n",
      "   relevance  clarity_q  specificity_q  ...  \\\n",
      "0          5          4              4  ...   \n",
      "1          5          4              4  ...   \n",
      "2          5          4              4  ...   \n",
      "3          5          4              4  ...   \n",
      "4          5          4              4  ...   \n",
      "\n",
      "                                      improved_query  query_intent  \\\n",
      "0  Welche Unterlagen sind erforderlich, um den Kl...  requirements   \n",
      "1  Welche Unterlagen sind für den Antrag des Klim...  requirements   \n",
      "2  Welche Unterlagen sind für die Beantragung des...  requirements   \n",
      "3  Welche Unterlagen sind für die Beantragung des...  requirements   \n",
      "4  Welche Unterlagen sind für die Beantragung des...  requirements   \n",
      "\n",
      "                                            keyterms score  \\\n",
      "0  [\"Dokumente\", \"Beantragung\", \"Klimageschwindig...     5   \n",
      "1  [\"Dokumente\", \"Unterlagen\", \"Antragsunterlagen...     4   \n",
      "2  [\"Unterlagen\", \"Klimageschwindigkeits-Bonus\", ...     4   \n",
      "3  [\"Klimageschwindigkeits-Bonus\", \"Beantragung\",...     5   \n",
      "4  [\"Dokumente\", \"Beantragung\", \"Unterlagen\", \"Kl...     5   \n",
      "\n",
      "                                             comment used_context  \\\n",
      "0  The generated response is fully accurate and a...        False   \n",
      "1  The generated response aligns well with the Ex...        False   \n",
      "2  The generated response accurately lists the tw...        False   \n",
      "3  The response accurately lists the required doc...        False   \n",
      "4  The generated response is fully accurate and a...        False   \n",
      "\n",
      "  retrieved_context used_context_ext retrieved_context_ext total_context_ids  \n",
      "0             False            False                 False                34  \n",
      "1             False            False                 False                37  \n",
      "2             False            False                 False                84  \n",
      "3              True            False                  True                33  \n",
      "4              True            False                  True                32  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "Index(['id_question', 'id_sample', 'type_question', 'question',\n",
      "       'expected_answer', 'clarity', 'specificity', 'relevance', 'clarity_q',\n",
      "       'specificity_q', 'relevance_q', 'prompt_id', 'device', 'test_name',\n",
      "       'id_test_question', 'doc_type', 'source', 'content', 'id', 'metadata',\n",
      "       'answer', 'begin_date', 'end_date', 'completion_tokens',\n",
      "       'prompt_tokens', 'context_used', 'context_ids', 'context_ids_total',\n",
      "       'alternate_prompts', 'improved_query', 'query_intent', 'keyterms',\n",
      "       'score', 'comment', 'used_context', 'retrieved_context',\n",
      "       'used_context_ext', 'retrieved_context_ext', 'total_context_ids'],\n",
      "      dtype='object')\n",
      "(2768, 39)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the root directory of your project\n",
    "project_root = '/Users/rodolfocacacho/Documents/Documents/MAI/Master Thesis/Code/rag_project'\n",
    "os.chdir(project_root)\n",
    "# Add the root directory to sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from config import (CONFIG_SQL_DB,DB_NAME,\n",
    "                    SQL_EVAL_QAS_TABLE_SCHEMA,\n",
    "                    SQL_EVAL_QAS_TABLE, \n",
    "                    EMBEDDING_MODEL,EMBEDDING_MODEL_API,\n",
    "                    EMBEDDING_MODEL_EMB_TASK,\n",
    "                    TEST_RESULTS_TABLE,SQL_EVAL_CHUNKS_TABLE,\n",
    "                    SQL_PROMPTS_TABLE,TEST_GEN_ANSWERS_TABLE)\n",
    "from utils.MySQLDB_manager import MySQLDB\n",
    "from testing.modules.evaluating_modules import RAGEvaluator\n",
    "import json\n",
    "\n",
    "sql_con = MySQLDB(CONFIG_SQL_DB,DB_NAME)\n",
    "\n",
    "ragEval = RAGEvaluator(sql_con=sql_con,\n",
    "                       test_table_name=TEST_RESULTS_TABLE,\n",
    "                       qas_table_name=SQL_EVAL_QAS_TABLE,\n",
    "                       chunks_eval_table_name=SQL_EVAL_CHUNKS_TABLE,\n",
    "                       prompts_table_name=SQL_PROMPTS_TABLE,\n",
    "                       eval_answers_table=TEST_GEN_ANSWERS_TABLE)\n",
    "\n",
    "df_results = ragEval.data_df\n",
    "\n",
    "print(df_results.head(5))\n",
    "print(df_results.columns)\n",
    "print(df_results.shape)\n",
    "\n",
    "# print(ragEval.generate_report())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_test_report(data_df, groupby_cols=None, output_file=\"test_report.xlsx\"):\n",
    "    \"\"\"\n",
    "    Generate detailed test reports with multiple groups and export them to an Excel file.\n",
    "    \n",
    "    Args:\n",
    "        data_df (pd.DataFrame): The dataframe containing the test data.\n",
    "        groupby_cols (list of str): Columns to create individual groupings for each test.\n",
    "        output_file (str): Path to the output Excel file.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if groupby_cols is None:\n",
    "        groupby_cols = []\n",
    "\n",
    "    # Start writing to Excel\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        # Group by 'test_name'\n",
    "        for test_name, test_group in data_df.groupby(\"test_name\"):\n",
    "            # General stats for the test (without grouping)\n",
    "            total_Qs = len(test_group)\n",
    "            avg_prompt_tokens = test_group['prompt_tokens'].mean()\n",
    "            std_prompt_tokens = test_group['prompt_tokens'].std()\n",
    "            avg_answer_tokens = test_group['completion_tokens'].mean()\n",
    "            std_answer_tokens = test_group['completion_tokens'].std()\n",
    "            avg_proc_time = (test_group['end_date'] - test_group['begin_date']).mean().total_seconds()\n",
    "            std_proc_time = (test_group['end_date'] - test_group['begin_date']).std().total_seconds()\n",
    "\n",
    "            general_stats = pd.DataFrame([{\n",
    "                'Total Questions': total_Qs,\n",
    "                'Avg Prompt Tokens': avg_prompt_tokens,\n",
    "                'Std Prompt Tokens': std_prompt_tokens,\n",
    "                'Avg Answer Tokens': avg_answer_tokens,\n",
    "                'Std Answer Tokens': std_answer_tokens,\n",
    "                'Avg Processing Time (s)': avg_proc_time,\n",
    "                'Std Processing Time (s)': std_proc_time,\n",
    "            }])\n",
    "            general_stats.to_excel(writer, sheet_name=f\"{test_name}_General\", index=False)\n",
    "\n",
    "            # Individual groupings for each column in `groupby_cols`\n",
    "            for col in groupby_cols:\n",
    "                group_stats = []\n",
    "                for group, group_data in test_group.groupby(col):\n",
    "                    total_Qs = len(group_data)\n",
    "                    avg_prompt_tokens = group_data['prompt_tokens'].mean()\n",
    "                    std_prompt_tokens = group_data['prompt_tokens'].std()\n",
    "                    avg_answer_tokens = group_data['completion_tokens'].mean()\n",
    "                    std_answer_tokens = group_data['completion_tokens'].std()\n",
    "                    avg_proc_time = (group_data['end_date'] - group_data['begin_date']).mean().total_seconds()\n",
    "                    std_proc_time = (group_data['end_date'] - group_data['begin_date']).std().total_seconds()\n",
    "\n",
    "                    group_stats.append({\n",
    "                        col: group,\n",
    "                        'Total Questions': total_Qs,\n",
    "                        'Avg Prompt Tokens': avg_prompt_tokens,\n",
    "                        'Std Prompt Tokens': std_prompt_tokens,\n",
    "                        'Avg Answer Tokens': avg_answer_tokens,\n",
    "                        'Std Answer Tokens': std_answer_tokens,\n",
    "                        'Avg Processing Time (s)': avg_proc_time,\n",
    "                        'Std Processing Time (s)': std_proc_time,\n",
    "                    })\n",
    "\n",
    "                # Convert group stats to DataFrame and write to a sheet\n",
    "                group_stats_df = pd.DataFrame(group_stats)\n",
    "                sheet_name = f\"{test_name}_{col[:25]}\"  # Ensure sheet name fits Excel's 31-char limit\n",
    "                group_stats_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Report saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id_question', 'id_sample', 'type_question', 'question',\n",
      "       'expected_answer', 'clarity', 'specificity', 'relevance', 'clarity_q',\n",
      "       'specificity_q', 'relevance_q', 'prompt_id', 'device', 'test_name',\n",
      "       'doc_type', 'source', 'content', 'id', 'metadata', 'answer',\n",
      "       'begin_date', 'end_date', 'completion_tokens', 'prompt_tokens',\n",
      "       'context_used', 'context_ids', 'context_ids_total', 'alternate_prompts',\n",
      "       'improved_query', 'query_intent', 'keyterms'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_results.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report saved to test_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "generate_test_report(df_results,groupby_cols=['type_question','doc_type','query_intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def calculate_custom_metrics(data_df):\n",
    "    \"\"\"\n",
    "    Preprocess the dataframe by adding custom metric columns.\n",
    "    \"\"\"\n",
    "    def get_adj_ids(id, last_id, dif=1):\n",
    "        adjs_ids = []\n",
    "        base, cid = id.split('.')\n",
    "        _, lid = last_id.split('.')\n",
    "        cid = int(cid)\n",
    "        lid = int(lid)\n",
    "        for offset in range(-dif, dif + 1):\n",
    "            adj_cid = cid + offset\n",
    "            if 0 <= adj_cid <= lid and adj_cid != cid:\n",
    "                adjs_ids.append(f\"{base}.{adj_cid}\")\n",
    "        return adjs_ids\n",
    "\n",
    "    def row_metrics(row):\n",
    "        metadata = json.loads(row['metadata'])\n",
    "        end_chunk = metadata['last_id']\n",
    "        context_ids = json.loads(row['context_ids'])\n",
    "        total_context_ids = json.loads(row['context_ids_total'])\n",
    "        adj_context = get_adj_ids(row['id'], end_chunk, dif=1)\n",
    "        adj_context.append(row['id'])\n",
    "        # Calculate binary score\n",
    "        binary_score = 1 if row['score'] > 3 else 0\n",
    "\n",
    "        return {\n",
    "            'used_context': row['id'] in context_ids,\n",
    "            'retrieved_context': row['id'] in total_context_ids,\n",
    "            'used_context_ext': any(item in adj_context for item in context_ids),\n",
    "            'retrieved_context_ext': any(item in adj_context for item in total_context_ids),\n",
    "            'total_context_ids': len(total_context_ids),\n",
    "            'binary_score': binary_score\n",
    "        }\n",
    "\n",
    "    # Apply the row-wise custom metric calculations\n",
    "    metrics = data_df.apply(row_metrics, axis=1)\n",
    "    metrics_df = pd.DataFrame(metrics.tolist())  # Convert list of dicts to a DataFrame\n",
    "    return pd.concat([data_df.reset_index(drop=True), metrics_df], axis=1)\n",
    "\n",
    "def generate_compact_test_report(data_df, groupby_cols=None, output_file=\"test_report.xlsx\"):\n",
    "    \"\"\"\n",
    "    Generate test reports with dynamic metric calculation and export to Excel.\n",
    "    \n",
    "    Args:\n",
    "        data_df (pd.DataFrame): The dataframe containing test data with precomputed metrics.\n",
    "        groupby_cols (list of str): Columns to group by within each test.\n",
    "        output_file (str): Path to the output Excel file.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if groupby_cols is None:\n",
    "        groupby_cols = []\n",
    "\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        for test_name, test_group in data_df.groupby(\"test_name\"):\n",
    "            # General Stats\n",
    "            general_stats = pd.DataFrame([{\n",
    "                'Total Questions': len(test_group),\n",
    "                'Avg Prompt Tokens': test_group['prompt_tokens'].mean(),\n",
    "                'Std Prompt Tokens': test_group['prompt_tokens'].std(),\n",
    "                'Avg Answer Tokens': test_group['completion_tokens'].mean(),\n",
    "                'Std Answer Tokens': test_group['completion_tokens'].std(),\n",
    "                'Retrieval Accuracy (used)': test_group['used_context'].mean(),\n",
    "                'Retrieval Accuracy (retrieved)': test_group['retrieved_context'].mean(),\n",
    "                'Expanded Retrieval Accuracy (used)': test_group['used_context_ext'].mean(),\n",
    "                'Expanded Retrieval Accuracy (retrieved)': test_group['retrieved_context_ext'].mean()\n",
    "            }])\n",
    "            general_stats.to_excel(writer, sheet_name=f\"{test_name}_General\", index=False)\n",
    "\n",
    "            # Individual Groupings\n",
    "            for col in groupby_cols:\n",
    "                group_stats = []\n",
    "                for group, group_data in test_group.groupby(col):\n",
    "                    group_stats.append({\n",
    "                        col: group,\n",
    "                        'Total Questions': len(group_data),\n",
    "                        'Avg Prompt Tokens': group_data['prompt_tokens'].mean(),\n",
    "                        'Std Prompt Tokens': group_data['prompt_tokens'].std(),\n",
    "                        'Avg Answer Tokens': group_data['completion_tokens'].mean(),\n",
    "                        'Std Answer Tokens': group_data['completion_tokens'].std(),\n",
    "                        'Retrieval Accuracy (used)': group_data['used_context'].mean(),\n",
    "                        'Retrieval Accuracy (retrieved)': group_data['retrieved_context'].mean(),\n",
    "                        'Expanded Retrieval Accuracy (used)': group_data['used_context_ext'].mean(),\n",
    "                        'Expanded Retrieval Accuracy (retrieved)': group_data['retrieved_context_ext'].mean()\n",
    "                    })\n",
    "\n",
    "                group_stats_df = pd.DataFrame(group_stats)\n",
    "                sheet_name = f\"{test_name}_{col[:25]}\"  # Ensure Excel sheet name limit\n",
    "                group_stats_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Report saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_consolidated_test_report(data_df, groupby_cols=None, output_file=\"consolidated_test_report.xlsx\"):\n",
    "    \"\"\"\n",
    "    Generate a consolidated test report with all tests in a single sheet for each group.\n",
    "\n",
    "    Args:\n",
    "        data_df (pd.DataFrame): The dataframe containing test data with precomputed metrics.\n",
    "        groupby_cols (list of str): Columns to group by for metrics.\n",
    "        output_file (str): Path to the output Excel file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if groupby_cols is None:\n",
    "        groupby_cols = []\n",
    "\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        # General Summary Sheet\n",
    "        summary_stats = []\n",
    "        for test_name, test_group in data_df.groupby(\"test_name\"):\n",
    "            summary_stats.append({\n",
    "                'Test Name': test_name,\n",
    "                'Total Questions': len(test_group),\n",
    "                'Avg Prompt Tokens': test_group['prompt_tokens'].mean(),\n",
    "                'Std Prompt Tokens': test_group['prompt_tokens'].std(),\n",
    "                'Avg Answer Tokens': test_group['completion_tokens'].mean(),\n",
    "                'Std Answer Tokens': test_group['completion_tokens'].std(),\n",
    "                'Retrieval Accuracy (used)': test_group['used_context'].mean(),\n",
    "                'Retrieval Accuracy (retrieved)': test_group['retrieved_context'].mean(),\n",
    "                'Expanded Retrieval Accuracy (used)': test_group['used_context_ext'].mean(),\n",
    "                'Expanded Retrieval Accuracy (retrieved)': test_group['retrieved_context_ext'].mean(),\n",
    "                'Avg Score': test_group['score'].mean(),\n",
    "                'Binary Score': test_group['binary_score'].mean()\n",
    "            })\n",
    "        summary_df = pd.DataFrame(summary_stats)\n",
    "        summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "\n",
    "        # Group Sheets\n",
    "        for col in groupby_cols:\n",
    "            group_stats = []\n",
    "            for test_name, test_group in data_df.groupby(\"test_name\"):\n",
    "                for group, group_data in test_group.groupby(col):\n",
    "                    group_stats.append({\n",
    "                        'Test Name': test_name,\n",
    "                        col: group,\n",
    "                        'Total Questions': len(group_data),\n",
    "                        'Avg Prompt Tokens': group_data['prompt_tokens'].mean(),\n",
    "                        'Std Prompt Tokens': group_data['prompt_tokens'].std(),\n",
    "                        'Avg Answer Tokens': group_data['completion_tokens'].mean(),\n",
    "                        'Std Answer Tokens': group_data['completion_tokens'].std(),\n",
    "                        'Retrieval Accuracy (used)': group_data['used_context'].mean(),\n",
    "                        'Retrieval Accuracy (retrieved)': group_data['retrieved_context'].mean(),\n",
    "                        'Expanded Retrieval Accuracy (used)': group_data['used_context_ext'].mean(),\n",
    "                        'Expanded Retrieval Accuracy (retrieved)': group_data['retrieved_context_ext'].mean(),\n",
    "                        'Avg Score': test_group['score'].mean(),\n",
    "                        'Binary Score': test_group['binary_score'].mean()\n",
    "                    })\n",
    "\n",
    "            # Convert to DataFrame and write to a single sheet per group\n",
    "            group_stats_df = pd.DataFrame(group_stats)\n",
    "            sheet_name = col[:31]  # Ensure sheet name fits Excel's character limit\n",
    "            group_stats_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Consolidated report saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report saved to test_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data to calculate metrics\n",
    "processed_df = calculate_custom_metrics(df_results)\n",
    "\n",
    "generate_compact_test_report(processed_df,groupby_cols=['type_question','doc_type','query_intent'],)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated report saved to consolidated_test_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "generate_consolidated_test_report(processed_df,groupby_cols=['type_question','doc_type','query_intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_question</th>\n",
       "      <th>id_sample</th>\n",
       "      <th>type_question</th>\n",
       "      <th>question</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>clarity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>relevance</th>\n",
       "      <th>clarity_q</th>\n",
       "      <th>specificity_q</th>\n",
       "      <th>...</th>\n",
       "      <th>improved_query</th>\n",
       "      <th>query_intent</th>\n",
       "      <th>keyterms</th>\n",
       "      <th>score</th>\n",
       "      <th>comment</th>\n",
       "      <th>used_context</th>\n",
       "      <th>retrieved_context</th>\n",
       "      <th>used_context_ext</th>\n",
       "      <th>retrieved_context_ext</th>\n",
       "      <th>total_context_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Factual</td>\n",
       "      <td>Welche Dokumente müssen bei der Beantragung de...</td>\n",
       "      <td>Bei der Beantragung des Klimageschwindigkeits-...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Welche Unterlagen sind erforderlich, um den Kl...</td>\n",
       "      <td>requirements</td>\n",
       "      <td>[\"Dokumente\", \"Beantragung\", \"Klimageschwindig...</td>\n",
       "      <td>5</td>\n",
       "      <td>The generated response is fully accurate and a...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Factual</td>\n",
       "      <td>Welche Dokumente müssen bei der Beantragung de...</td>\n",
       "      <td>Bei der Beantragung des Klimageschwindigkeits-...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Welche Unterlagen sind für den Antrag des Klim...</td>\n",
       "      <td>requirements</td>\n",
       "      <td>[\"Dokumente\", \"Unterlagen\", \"Antragsunterlagen...</td>\n",
       "      <td>4</td>\n",
       "      <td>The generated response aligns well with the Ex...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Factual</td>\n",
       "      <td>Welche Dokumente müssen bei der Beantragung de...</td>\n",
       "      <td>Bei der Beantragung des Klimageschwindigkeits-...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Welche Unterlagen sind für die Beantragung des...</td>\n",
       "      <td>requirements</td>\n",
       "      <td>[\"Unterlagen\", \"Klimageschwindigkeits-Bonus\", ...</td>\n",
       "      <td>4</td>\n",
       "      <td>The generated response accurately lists the tw...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Factual</td>\n",
       "      <td>Welche Dokumente müssen bei der Beantragung de...</td>\n",
       "      <td>Bei der Beantragung des Klimageschwindigkeits-...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Welche Unterlagen sind für die Beantragung des...</td>\n",
       "      <td>requirements</td>\n",
       "      <td>[\"Klimageschwindigkeits-Bonus\", \"Beantragung\",...</td>\n",
       "      <td>5</td>\n",
       "      <td>The response accurately lists the required doc...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Factual</td>\n",
       "      <td>Welche Dokumente müssen bei der Beantragung de...</td>\n",
       "      <td>Bei der Beantragung des Klimageschwindigkeits-...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Welche Unterlagen sind für die Beantragung des...</td>\n",
       "      <td>requirements</td>\n",
       "      <td>[\"Dokumente\", \"Beantragung\", \"Unterlagen\", \"Kl...</td>\n",
       "      <td>5</td>\n",
       "      <td>The generated response is fully accurate and a...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>635</td>\n",
       "      <td>127</td>\n",
       "      <td>Analytical</td>\n",
       "      <td>Warum ist es notwendig, eine tabellarische Bel...</td>\n",
       "      <td>1. **Notwendigkeit der tabellarischen Belegübe...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>Weshalb muss bei der Einreichung des Verwendun...</td>\n",
       "      <td>requirements</td>\n",
       "      <td>[\"Ausgabenübersicht\", \"Verantwortungsnachweis\"...</td>\n",
       "      <td>4</td>\n",
       "      <td>The generated response accurately covers the n...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>635</td>\n",
       "      <td>127</td>\n",
       "      <td>Analytical</td>\n",
       "      <td>Warum ist es notwendig, eine tabellarische Bel...</td>\n",
       "      <td>1. **Notwendigkeit der tabellarischen Belegübe...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>Welche Informationen muss eine tabellarische B...</td>\n",
       "      <td>compliance</td>\n",
       "      <td>[\"Belegübersicht\", \"Verwendungsnachweis\", \"Bun...</td>\n",
       "      <td>5</td>\n",
       "      <td>The generated response accurately captures the...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>635</td>\n",
       "      <td>127</td>\n",
       "      <td>Analytical</td>\n",
       "      <td>Warum ist es notwendig, eine tabellarische Bel...</td>\n",
       "      <td>1. **Notwendigkeit der tabellarischen Belegübe...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>Welche Informationen muss die tabellarische Be...</td>\n",
       "      <td>requirements</td>\n",
       "      <td>[\"tabellarische Belegübersicht\", \"Einreichung\"...</td>\n",
       "      <td>4</td>\n",
       "      <td>The generated response is largely accurate and...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>635</td>\n",
       "      <td>127</td>\n",
       "      <td>Analytical</td>\n",
       "      <td>Warum ist es notwendig, eine tabellarische Bel...</td>\n",
       "      <td>1. **Notwendigkeit der tabellarischen Belegübe...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>Warum muss eine tabellarische Belegübersicht b...</td>\n",
       "      <td>compliance</td>\n",
       "      <td>[\"Verwendungsnachweis\", \"Tabelle der Belege\", ...</td>\n",
       "      <td>5</td>\n",
       "      <td>The generated response is fully accurate and a...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>635</td>\n",
       "      <td>127</td>\n",
       "      <td>Analytical</td>\n",
       "      <td>Warum ist es notwendig, eine tabellarische Bel...</td>\n",
       "      <td>1. **Notwendigkeit der tabellarischen Belegübe...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>Welche Gründe gibt es für die Erstellung einer...</td>\n",
       "      <td>compliance</td>\n",
       "      <td>[\"Dokumentationsübersicht\", \"tabellarische Bel...</td>\n",
       "      <td>4</td>\n",
       "      <td>The generated response is accurate and aligns ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2768 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_question  id_sample type_question  \\\n",
       "0               1          1       Factual   \n",
       "1               1          1       Factual   \n",
       "2               1          1       Factual   \n",
       "3               1          1       Factual   \n",
       "4               1          1       Factual   \n",
       "...           ...        ...           ...   \n",
       "2763          635        127    Analytical   \n",
       "2764          635        127    Analytical   \n",
       "2765          635        127    Analytical   \n",
       "2766          635        127    Analytical   \n",
       "2767          635        127    Analytical   \n",
       "\n",
       "                                               question  \\\n",
       "0     Welche Dokumente müssen bei der Beantragung de...   \n",
       "1     Welche Dokumente müssen bei der Beantragung de...   \n",
       "2     Welche Dokumente müssen bei der Beantragung de...   \n",
       "3     Welche Dokumente müssen bei der Beantragung de...   \n",
       "4     Welche Dokumente müssen bei der Beantragung de...   \n",
       "...                                                 ...   \n",
       "2763  Warum ist es notwendig, eine tabellarische Bel...   \n",
       "2764  Warum ist es notwendig, eine tabellarische Bel...   \n",
       "2765  Warum ist es notwendig, eine tabellarische Bel...   \n",
       "2766  Warum ist es notwendig, eine tabellarische Bel...   \n",
       "2767  Warum ist es notwendig, eine tabellarische Bel...   \n",
       "\n",
       "                                        expected_answer  clarity  specificity  \\\n",
       "0     Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
       "1     Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
       "2     Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
       "3     Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
       "4     Bei der Beantragung des Klimageschwindigkeits-...        5            5   \n",
       "...                                                 ...      ...          ...   \n",
       "2763  1. **Notwendigkeit der tabellarischen Belegübe...        4            5   \n",
       "2764  1. **Notwendigkeit der tabellarischen Belegübe...        4            5   \n",
       "2765  1. **Notwendigkeit der tabellarischen Belegübe...        4            5   \n",
       "2766  1. **Notwendigkeit der tabellarischen Belegübe...        4            5   \n",
       "2767  1. **Notwendigkeit der tabellarischen Belegübe...        4            5   \n",
       "\n",
       "      relevance  clarity_q  specificity_q  ...  \\\n",
       "0             5          4              4  ...   \n",
       "1             5          4              4  ...   \n",
       "2             5          4              4  ...   \n",
       "3             5          4              4  ...   \n",
       "4             5          4              4  ...   \n",
       "...         ...        ...            ...  ...   \n",
       "2763          5          5              5  ...   \n",
       "2764          5          5              5  ...   \n",
       "2765          5          5              5  ...   \n",
       "2766          5          5              5  ...   \n",
       "2767          5          5              5  ...   \n",
       "\n",
       "                                         improved_query  query_intent  \\\n",
       "0     Welche Unterlagen sind erforderlich, um den Kl...  requirements   \n",
       "1     Welche Unterlagen sind für den Antrag des Klim...  requirements   \n",
       "2     Welche Unterlagen sind für die Beantragung des...  requirements   \n",
       "3     Welche Unterlagen sind für die Beantragung des...  requirements   \n",
       "4     Welche Unterlagen sind für die Beantragung des...  requirements   \n",
       "...                                                 ...           ...   \n",
       "2763  Weshalb muss bei der Einreichung des Verwendun...  requirements   \n",
       "2764  Welche Informationen muss eine tabellarische B...    compliance   \n",
       "2765  Welche Informationen muss die tabellarische Be...  requirements   \n",
       "2766  Warum muss eine tabellarische Belegübersicht b...    compliance   \n",
       "2767  Welche Gründe gibt es für die Erstellung einer...    compliance   \n",
       "\n",
       "                                               keyterms score  \\\n",
       "0     [\"Dokumente\", \"Beantragung\", \"Klimageschwindig...     5   \n",
       "1     [\"Dokumente\", \"Unterlagen\", \"Antragsunterlagen...     4   \n",
       "2     [\"Unterlagen\", \"Klimageschwindigkeits-Bonus\", ...     4   \n",
       "3     [\"Klimageschwindigkeits-Bonus\", \"Beantragung\",...     5   \n",
       "4     [\"Dokumente\", \"Beantragung\", \"Unterlagen\", \"Kl...     5   \n",
       "...                                                 ...   ...   \n",
       "2763  [\"Ausgabenübersicht\", \"Verantwortungsnachweis\"...     4   \n",
       "2764  [\"Belegübersicht\", \"Verwendungsnachweis\", \"Bun...     5   \n",
       "2765  [\"tabellarische Belegübersicht\", \"Einreichung\"...     4   \n",
       "2766  [\"Verwendungsnachweis\", \"Tabelle der Belege\", ...     5   \n",
       "2767  [\"Dokumentationsübersicht\", \"tabellarische Bel...     4   \n",
       "\n",
       "                                                comment used_context  \\\n",
       "0     The generated response is fully accurate and a...        False   \n",
       "1     The generated response aligns well with the Ex...        False   \n",
       "2     The generated response accurately lists the tw...        False   \n",
       "3     The response accurately lists the required doc...        False   \n",
       "4     The generated response is fully accurate and a...        False   \n",
       "...                                                 ...          ...   \n",
       "2763  The generated response accurately covers the n...        False   \n",
       "2764  The generated response accurately captures the...         True   \n",
       "2765  The generated response is largely accurate and...         True   \n",
       "2766  The generated response is fully accurate and a...         True   \n",
       "2767  The generated response is accurate and aligns ...        False   \n",
       "\n",
       "     retrieved_context used_context_ext retrieved_context_ext  \\\n",
       "0                False            False                 False   \n",
       "1                False            False                 False   \n",
       "2                False            False                 False   \n",
       "3                 True            False                  True   \n",
       "4                 True            False                  True   \n",
       "...                ...              ...                   ...   \n",
       "2763             False            False                  True   \n",
       "2764              True             True                  True   \n",
       "2765              True             True                  True   \n",
       "2766              True             True                  True   \n",
       "2767              True            False                  True   \n",
       "\n",
       "     total_context_ids  \n",
       "0                   34  \n",
       "1                   37  \n",
       "2                   84  \n",
       "3                   33  \n",
       "4                   32  \n",
       "...                ...  \n",
       "2763                57  \n",
       "2764                56  \n",
       "2765                63  \n",
       "2766                39  \n",
       "2767                75  \n",
       "\n",
       "[2768 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_unstructured",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
