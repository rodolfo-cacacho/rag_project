{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rodolfocacacho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load German stopwords and the German spaCy model\n",
    "nltk.download('stopwords')\n",
    "german_stopwords = set(stopwords.words('german'))\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Remove line breaks and standalone special characters from text.\"\"\"\n",
    "    # Replace line breaks with a space\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove standalone special characters surrounded by spaces\n",
    "    text = re.sub(r'\\s[-/]\\s', ' ', text)  # Example for \" - \" and \" / \"\n",
    "    \n",
    "    # Remove extra spaces that may have been created\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_urls_and_entities(text):\n",
    "    # Extract URLs\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, '')  # Remove URLs from text\n",
    "\n",
    "    # Extract standard entities with spaCy\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    for entity in entities:\n",
    "        text = text.replace(entity, '')  # Remove entities from text\n",
    "\n",
    "    return text, urls, entities\n",
    "\n",
    "def isolate_number_unit_terms(text):\n",
    "    \"\"\"Find and remove terms with a number followed by a unit (e.g., '525 kWh/m2').\"\"\"\n",
    "    number_unit_pattern = r'\\b\\d+(?:[\\.,]\\d+)?\\s*(?:kwh/m2|°c|m2|g|kg|l|ml|w|kw|v|a)\\b'\n",
    "    number_unit_terms = re.findall(number_unit_pattern, text, re.IGNORECASE)\n",
    "    for term in number_unit_terms:\n",
    "        text = text.replace(term, '')  # Remove terms with numbers and units\n",
    "\n",
    "    return text, number_unit_terms\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9äöüÄÖÜß\\s\\-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def find_chapters_and_sections(text):\n",
    "    \"\"\"\n",
    "    Identifies chapter and section numbers in a given text, such as '3.03', '6.1.4.3', etc.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: str, input text to search for sections.\n",
    "    \n",
    "    Returns:\n",
    "    - List of chapter/section numbers found in the text.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to capture chapter/section numbers\n",
    "    section_pattern = r'\\b\\d+(?:\\.\\d+)+\\b'\n",
    "    \n",
    "    # Find all matches of the pattern in the text\n",
    "    sections = re.findall(section_pattern, text)\n",
    "    for term in sections:\n",
    "        text = text.replace(term,'')\n",
    "    \n",
    "    return text,sections\n",
    "\n",
    "def find_norms(text):\n",
    "    \"\"\"\n",
    "    Identifies norms like DIN, ISO, EN, etc., with their associated names, numbers, and variants.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: str, input text to search for norms.\n",
    "    \n",
    "    Returns:\n",
    "    - List of norms found in the text.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to capture norms\n",
    "    norm_pattern = r'\\b(?:DIN|ISO|EN)(?:\\s+[A-Z]{1,3})?\\s+\\d{3,5}(?:[-‑]\\d+)?\\b'\n",
    "    \n",
    "    # Find all matches of the pattern in the text\n",
    "    norms = re.findall(norm_pattern, text, re.IGNORECASE)\n",
    "    for term in norms:\n",
    "        text = text.replace(term,'')\n",
    "    \n",
    "    return text,norms\n",
    "\n",
    "def find_short_terms(text):\n",
    "    \"\"\"\n",
    "    Identifies short terms like 'WG' or 'NWG' that are 2-3 characters long and uppercase.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: str, input text to search for short terms.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple: modified text (with short terms removed), list of short terms found.\n",
    "    \"\"\"\n",
    "    short_term_pattern = r'\\b[A-Z]{2,3}\\b'\n",
    "    short_terms = re.findall(short_term_pattern, text)\n",
    "    \n",
    "    for term in short_terms:\n",
    "        text = text.replace(term, '')\n",
    "        \n",
    "    return text, short_terms\n",
    "\n",
    "def clean_chunk(text):\n",
    "    # Step 1: Preprocess text to remove line breaks and standalone special characters\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    # Step 2: Extract URLs, entities, and number-unit terms, removing them from text\n",
    "    text,sections = find_chapters_and_sections(text)\n",
    "    text,norms = find_norms(text)\n",
    "    text,short_terms = find_short_terms(text)\n",
    "    text, number_unit_terms = isolate_number_unit_terms(text)\n",
    "    text, urls, entities = extract_urls_and_entities(text)\n",
    "    # Step 3: Clean text to remove unwanted characters and extra spaces\n",
    "    text = clean_text(text)\n",
    "\n",
    "    # Step 4: Tokenize, remove stopwords, and perform lemmatization using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Step 5: Filter out stopwords and short words (words with length <= 2)\n",
    "    cleaned_tokens = [\n",
    "        token.lemma_.lower() for token in doc \n",
    "        if token.text.lower() not in german_stopwords and len(token.text) > 2\n",
    "    ]\n",
    "\n",
    "    # Step 6: Extend the tokens with extracted URLs, entities, and number-unit terms\n",
    "    cleaned_tokens.extend(short_terms)\n",
    "    cleaned_tokens.extend(sections)\n",
    "    cleaned_tokens.extend(norms)\n",
    "    cleaned_tokens.extend(urls)\n",
    "    cleaned_tokens.extend(entities)\n",
    "    cleaned_tokens.extend(number_unit_terms)\n",
    "    cleaned_tokens = [token.lower() for token in cleaned_tokens]\n",
    "\n",
    "    # Step 7: Return the final list of tokens\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Erfolgt eine Maßnahme im Rahmen der Umsetzung eines im Förderprogramm „Bundesförderung für Energieberatung für Wohngebäude“ geförderten iSFP, und wurde dies im Rahmen des Antrags nach Nummer 9.2 vom Antragsteller gekennzeichnet, prüft der Energieeffizienz-Experte im Rahmen der Prüfung des Antrags auch, ob die beantragte Maßnahme dem iSFP entspricht und sie daher als iSFP-Maßnahme gewertet werden kann; unwesentliche inhaltliche Abweichungen, eine Übererfüllung der iSFP-Vorgaben oder Änderungen der zeitlichen Reihenfolge sind dabei unschädlich.\n",
    "Abweichungen von der im Zuwendungsbescheid bzw. in der Zusage bewilligten Maßnahme sind dem BAFA bzw. der KfW unverzüglich anzuzeigen. Liegt eine wesentliche inhaltliche Abweichung im Sinne einer Untererfüllung der iSFP-Vorgaben vor, kann die Maßnahme nicht als iSFP-Maßnahme gewertet werden.\n",
    "9.4.1 Zuschussförderung\n",
    "Eine Zuschussförderung wird nur befristet zugesagt. Die Dauer der Befristung beträgt 24 Monate ab Zugang der Zusage des Zuwendungsbescheids (Bewilligungszeitraum). Die Befristung kann auf begründeten Antrag um maximal 24 Monate verlängert werden, wenn die Umsetzung der Maßnahme innerhalb der ursprünglichen Frist vom Antragsteller aus Gründen nicht umgesetzt werden konnte, die der Antragsteller nicht zu vertreten hat.\n",
    "Die maximale Bewilligungsfrist für Einzelmaßnahmen beträgt damit 48 Monate.\n",
    "9.4.2 Kreditförderung\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['erfolgen',\n",
       " 'maßnahme',\n",
       " 'rahmen',\n",
       " 'umsetzung',\n",
       " 'förderprogramm',\n",
       " 'bundesförderung',\n",
       " 'energieberatung',\n",
       " 'wohngebäude',\n",
       " 'geförderen',\n",
       " 'werden',\n",
       " 'rahmen',\n",
       " 'nummer',\n",
       " 'teller',\n",
       " 'kennzeichnen',\n",
       " 'prüfen',\n",
       " 'energieeffizienz-experte',\n",
       " 'rahmen',\n",
       " 'prüfung',\n",
       " 'beantragt',\n",
       " 'maßnahme',\n",
       " 'entsprechen',\n",
       " 'daher',\n",
       " '-maßnahme',\n",
       " 'werten',\n",
       " 'unwesentlich',\n",
       " 'inhaltlich',\n",
       " 'abweichung',\n",
       " 'übererfüllung',\n",
       " 'vorgab',\n",
       " 'änderung',\n",
       " 'zeitlich',\n",
       " 'reihenfolge',\n",
       " 'dabei',\n",
       " 'unschädlich',\n",
       " 'abweichung',\n",
       " 'zuwendungsbescheid',\n",
       " 'bzw',\n",
       " 'zusage',\n",
       " 'bewilligt',\n",
       " 'maßnahme',\n",
       " 'bzw',\n",
       " 'unverzüglich',\n",
       " 'anzuzeig',\n",
       " 'leegten',\n",
       " 'wesentlich',\n",
       " 'inhaltlich',\n",
       " 'abweichung',\n",
       " 'sinn',\n",
       " 'untererfüllung',\n",
       " 'vorgaben',\n",
       " 'maßnahme',\n",
       " '-maßnahme',\n",
       " 'werten',\n",
       " 'zuschussförderung',\n",
       " 'zuschussförderung',\n",
       " 'befristet',\n",
       " 'zusagen',\n",
       " 'dauer',\n",
       " 'befristung',\n",
       " 'betragen',\n",
       " 'monat',\n",
       " 'zugang',\n",
       " 'zusage',\n",
       " 'zuwendungsbescheid',\n",
       " 'bewilligungszeitraum',\n",
       " 'befristung',\n",
       " 'begründet',\n",
       " 'antrag',\n",
       " 'maximal',\n",
       " 'monat',\n",
       " 'verlängern',\n",
       " 'umsetzung',\n",
       " 'maßnahme',\n",
       " 'innerhalb',\n",
       " 'ursprünglich',\n",
       " 'frist',\n",
       " 'teller',\n",
       " 'grund',\n",
       " 'umsetzen',\n",
       " 'können',\n",
       " 'teller',\n",
       " 'vertreten',\n",
       " 'maximal',\n",
       " 'bewilligungsfrist',\n",
       " 'einzelmaßnahme',\n",
       " 'betragen',\n",
       " 'monat',\n",
       " 'kreditförderung',\n",
       " '9.2',\n",
       " '9.4.1',\n",
       " '9.4.2',\n",
       " 'antrags',\n",
       " 'isfp',\n",
       " 'bafa',\n",
       " 'kfw']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_chunk(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean text pre embedding with LLM/ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodolfocacacho/miniforge3/envs/rag_unstructured/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rodolfocacacho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata\n",
    "from chunking_embeding_docs import load_documents_pages,config as db_config,db_name,merge_pages\n",
    "from database_manager import MySQLDB\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import openai\n",
    "import base64\n",
    "from textwrap import dedent\n",
    "\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "dir_main = '/Users/rodolfocacacho/Documents/Documents/MAI/Master Thesis/Code/rag_clean_v2'\n",
    "os.chdir(dir_main)\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY_CGPT = os.getenv('API_KEY_CGPT')\n",
    "openai.api_key = API_KEY_CGPT\n",
    "\n",
    "\n",
    "metadata_csv = 'data/documents/metadata/Files_date_version.csv'\n",
    "df_codes = pd.read_csv(metadata_csv)\n",
    "df_codes['type_key'] = df_codes['type_key'].astype('int16')\n",
    "df_codes['file_key'] = df_codes['file_key'].astype('int16')\n",
    "# Convert document_date to datetime if it's not already\n",
    "df_codes['date_c'] = pd.to_datetime(df_codes['date'], format=\"%d/%m/%Y\", errors='coerce', dayfirst=True)\n",
    "\n",
    "# Sort by date to ensure the most recent appears last within each type_key\n",
    "df_codes = df_codes.sort_values(['type_key', 'date_c'], ascending=[True, False])\n",
    "\n",
    "# Identify the most recent document within each type_key\n",
    "df_codes['most_recent'] = df_codes.groupby('type_key')['date_c'].transform('max') == df_codes['date_c']\n",
    "\n",
    "path_store ='/Users/rodolfocacacho/Documents/Documents/MAI/Master Thesis/Code/rag_clean_v2/data/storage/embeddings'\n",
    "\n",
    "sql_embed_table = 'embedding_table_pinecone_sparse_new'\n",
    "new_docs = []\n",
    "\n",
    "sql_con = MySQLDB(config=db_config,database_name=db_name)\n",
    "docs = load_documents_pages(sql_con=sql_con)        \n",
    "\n",
    "complete_docs = []\n",
    "for j in docs:\n",
    "    pages_list = []\n",
    "    for i in j:\n",
    "        i['content'] = unicodedata.normalize('NFC',i['content'])\n",
    "        page = i['content']\n",
    "        pages_list.append(page)\n",
    "\n",
    "    merged_doc = merge_pages(pages_list)\n",
    "    complete_docs.append(merged_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Placeholder(BaseModel):\n",
    "    placeholder_type: str\n",
    "    text: str\n",
    "\n",
    "class Clean_text(BaseModel):\n",
    "    clean_text: str\n",
    "    placeholders: list[Placeholder]\n",
    "\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def call_gpt_api_with_single_prompt(instructions, prompt, model=\"gpt-4o-2024-08-06\", max_tokens=2500, response_format=None, img_path=None,detail='high'):\n",
    "    \"\"\"\n",
    "    Sends a single message to GPT API with optional image input and retrieves the response.\n",
    "    \n",
    "    Parameters:\n",
    "    - instructions: System instructions to set the context (e.g., \"You are an AI assistant that analyzes tables\").\n",
    "    - prompt: User's message or query (e.g., \"Please analyze the table in the image and provide a summary\").\n",
    "    - model: The GPT model to be used (default is \"gpt-4o-2024-08-06\").\n",
    "    - max_tokens: Maximum number of tokens for the response (default is 2500).\n",
    "    - response_format: Format of the response (e.g., \"Rag_reponse\"). Defaults to standard completion if not provided.\n",
    "    - img_path: Optional path to an image file. If provided, the image will be included in the request.\n",
    "    \n",
    "    Returns:\n",
    "    - The GPT answer object.\n",
    "    \"\"\"\n",
    "\n",
    "    content = []\n",
    "    dict_images = []\n",
    "    # Create the messages list to send to GPT\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions}\n",
    "    ]\n",
    "\n",
    "    # If an image path is provided, encode and append it as a separate message\n",
    "    if img_path:\n",
    "        base64_image = encode_image(img_path)\n",
    "        prompt_text = {'type':'text','text':dedent(prompt)}\n",
    "        dic_images = {'type':'image_url','image_url':{'url': f\"data:image/png;base64,{base64_image}\",'detail':detail}}\n",
    "        dict_images.append(dic_images)\n",
    "        content.append(prompt_text)\n",
    "        content.extend(dict_images)\n",
    "        chat = {\"role\": \"user\", \"content\":content}\n",
    "\n",
    "    else:\n",
    "        chat = {\"role\": \"user\", \"content\":dedent(prompt)}\n",
    "    \n",
    "    messages.append(chat)\n",
    "    \n",
    "    try:\n",
    "        if response_format == None:\n",
    "            # Call GPT API using OpenAI's beta chat completions with parse\n",
    "            response = openai.beta.chat.completions.parse(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens)\n",
    "        else:\n",
    "            # Call GPT API using OpenAI's beta chat completions with parse\n",
    "            response = openai.beta.chat.completions.parse(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            response_format=response_format)\n",
    "\n",
    "        # Extract and return the response content\n",
    "        answer = response.choices[0].message.content\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPT API call: {e}\")\n",
    "        return None\n",
    "\n",
    "instructions_cleaning_chunk = \"\"\"This is a system designed to clean and structure German-language texts related to regulatory guidelines, green financial aid for buildings, and similar topics. The objective is to improve semantic chunking and embedding accuracy by removing noise and irrelevant text while maintaining placeholders for essential information. This cleaning protocol ensures that only meaningful content is embedded, while retaining critical placeholders for terms, dates, and references commonly found in regulatory documents.\n",
    "\n",
    "Instructions\n",
    "\n",
    "\t1.\tGeneral Cleaning and Noise Removal:\n",
    "\t•\tRemove redundant punctuation (e.g., multiple periods ..., excessive spaces) and any symbols that don’t add meaning.\n",
    "\t•\tPreserve hyphens within compound words relevant to the document’s context, such as Energie-Effizienz.\n",
    "\t•\tDiscard formal or repetitive phrases that don’t contribute to semantic meaning, like introductory notes (e.g., “Wichtiger Hinweis”, “Bitte beachten Sie”).\n",
    "    •   Don't change the text, don't complete it and don't rephrase it, just clean what could add noise to the embeddings by removing it. You are given chunks that may not be complete.\n",
    "    •   While cleaning, retain line breaks where they logically divide sections or distinct topics within the text. If line breaks appear to separate unrelated content, such as a new topic or regulation clause, keep them intact. Only remove line breaks that do not contribute to the clarity or logical flow, such as multiple consecutive breaks or breaks within sentences.\n",
    "    •   For the Table of contents/Inhalt or similar, keep the line breaks. \n",
    "\t2.\tPlaceholder Masking (in German):\n",
    "\t•\tFor specific content types, replace with German-labeled placeholders. Follow this format:\n",
    "\t•\t[Datum] – For dates (e.g., 01.01.2021, 20.06.2023, not Datum/Daten).\n",
    "\t•\t[Wert] – For numeric values and measurements (e.g., 15 kWh, 50%, €1000).\n",
    "\t•\t[Version] – For version details (e.g., Version 2.0, v3.1, ignore when not accompanied by a number).\n",
    "\t•\t[Gesetz] – For legal references, such as § 35a, Artikel 27.\n",
    "\t•\t[Email] – For email addresses (e.g., kontakt@unternehmen.de).\n",
    "\t•\t[URL] – For URLs (e.g., http://www.beispiel.de).\n",
    "\t•\t[Abschnitt] – For section or chapter headings (e.g., Abschnitt 2.3, Kapitel 4.5).\n",
    "\t•\t[Telefon] – For phone numbers (e.g., 0800 123456).\n",
    "    •   [Jahr] - For years (e.g, 2020, 2 019, etc.)\n",
    "\t•\tAdditional Flexibility: The model should recognize other relevant parts and add placeholders for these as needed to enhance retrieval accuracy. Special cases could be equations, insitutions, etc.\n",
    "\t3.\tOutput Requirements:\n",
    "\t•\tPlaceholder List: Provide a List showing each placeholder and its corresponding original text, for instance, [Datum]: [\"01.01.2021\"].\n",
    "\t•\tReturn Clean Text: Supply the final text with placeholders in place, optimized for semantic embedding.\n",
    "    •   Make sure to match the number of placeholders in the text as in the list!!!\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def verify_chunk(text,placeholders):\n",
    "    placeholder_counts = Counter(item['placeholder_type'] for item in placeholders)\n",
    "    for placeholder_type, count in placeholder_counts.items():\n",
    "        count_in_text = text.count(f\"[{placeholder_type}]\")\n",
    "        \n",
    "        # Check for mismatch between counts\n",
    "        if count != count_in_text:\n",
    "            return False\n",
    "    \n",
    "    # If no mismatches are found, return True\n",
    "    return True       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Richtlinie BEG EM (2020-12-17).pdf recent:False\n",
      "1-Richtlinie BEG EM (2023-12-21).pdf recent:True\n",
      "2-Richtlinie BEG EM (2022-07-21)_Änderung.pdf recent:False\n",
      "3-Richtlinie BEG EM (2021-05-20).pdf recent:False\n",
      "4-Richtlinie BEG EM (2021-09-16).pdf recent:False\n",
      "5-Richtlinie BEG EM (2022-12-09).pdf recent:False\n",
      "6-Richtlinie BEG EM (2022-09-15)_Änderung.pdf recent:False\n",
      "7-Allgemeines Merkblatt zur Antragstellung - Zuschuss_1.9 (2024-01-01).pdf recent:False\n",
      "8-Allgemeines Merkblatt zur Antragstellung - Zuschuss_1.10 (2024-04-01).pdf recent:True\n",
      "9-Allgemeines Merkblatt zur Antragstellung - Zuschuss_1.6 (2023-01-01).pdf recent:False\n",
      "10-Allgemeines Merkblatt zur Antragstellung - Zuschuss_1.8 (2023-08-31).pdf recent:False\n",
      "11-Allgemeines Merkblatt zur Antragstellung - Zuschuss_1.7 (2023-01-01).pdf recent:False\n",
      "12-BEG Infoblatt förderfähigen Kosten_8 (20.06.2023).pdf recent:False\n",
      "13-BEG Infoblatt förderfähigen Kosten_3 (01.02.2022).pdf recent:False\n",
      "14-BEG Infoblatt förderfähigen Kosten_2 (21.10.2021).pdf recent:False\n",
      "15-BEG Infoblatt förderfähigen Kosten_6 (22.09.2022).pdf recent:False\n",
      "16-BEG Infoblatt förderfähigen Kosten_7 (01.01.2023).pdf recent:False\n",
      "17-BEG Infoblatt förderfähigen Kosten_5 (15.08.2022).pdf recent:False\n",
      "18-BEG Infoblatt förderfähigen Kosten_1 (01.05.2021).pdf recent:False\n",
      "19-BEG Infoblatt förderfähigen Kosten_9 (01.01.2024).pdf recent:True\n",
      "Page 0 done\n",
      "Page 1 done\n",
      "Page 2 done\n",
      "Page 3 done\n",
      "Page 4 done\n",
      "Page 5 done\n"
     ]
    }
   ],
   "source": [
    "doc_name = 'BEG Infoblatt förderfähigen Kosten_9 (01.01.2024).pdf'\n",
    "done_doc = False\n",
    "chunks_processed = []\n",
    "for i,doc in enumerate(docs):\n",
    "    metadata = doc[0]['metadata']\n",
    "    metadata = json.loads(metadata)\n",
    "    pdf_name = metadata[\"source\"]\n",
    "    matched_row = df_codes[df_codes['file'] == pdf_name]\n",
    "    if not matched_row.empty:\n",
    "        # Extract type_key and file_key from the matched row (assuming there's only one match)\n",
    "        recent = matched_row[\"most_recent\"].iloc[0]\n",
    "    else:\n",
    "        # If no match found, set type_key and file_key to 0\n",
    "        recent = False  \n",
    "    print(f'{i}-{pdf_name} recent:{recent}')\n",
    "\n",
    "    if recent and pdf_name == doc_name:\n",
    "        for num_page,page in enumerate(doc):\n",
    "            retries = 0\n",
    "            check = False\n",
    "            done_doc = True\n",
    "            page_content = page['content']\n",
    "            prompt_chunk = f'Process the following chunk:\\n{page_content}'\n",
    "\n",
    "            while not check:\n",
    "                if retries > 0:\n",
    "                    prompt_chunk_s = f'Make sure to match the number of placeholders in the text and the list returned. This is a retry. {prompt_chunk}'\n",
    "                    print(f'Retrying {retries}')\n",
    "                else:\n",
    "                    prompt_chunk_s = prompt_chunk\n",
    "                \n",
    "                answer = call_gpt_api_with_single_prompt(instructions_cleaning_chunk,\n",
    "                                                        prompt= prompt_chunk_s,\n",
    "                                                        max_tokens=4000,\n",
    "                                                        response_format=Clean_text)\n",
    "                answer = json.loads(answer)\n",
    "                check = verify_chunk(answer['clean_text'],answer['placeholders'])\n",
    "\n",
    "            if check:\n",
    "                print(f'Page {num_page} done')\n",
    "                answer['correct'] = True\n",
    "                answer['id'] = doc['id']\n",
    "                chunks_processed.append(answer)\n",
    "            else:\n",
    "                retries+=1\n",
    "                if retries > 2:\n",
    "                    wrong_answer = {'clean_text':page_content,\n",
    "                                    'placeholders': [],\n",
    "                                    'correct': False,\n",
    "                                    'id':doc['id']}\n",
    "                    chunks_processed.append(wrong_answer)\n",
    "                    check = True\n",
    "            if num_page > 4:\n",
    "                break\n",
    "    if done_doc:\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bundesförderung für effiziente Gebäude: Infoblatt zu den förderfähigen Maßnahmen und Leistungen - Sanieren Dieses Infoblatt zu den förderfähigen Maßnahmen und Leistungen – Sanieren ist zur Ermittlung der förderfähigen Kosten bei der Antragstellung sowie im Rahmen des Verwendungsnachweises anzuwenden. In den Kredit- oder Zuschussvarianten der BEG bei der [Institution] sind diese Kosten von der Energieeffizienz-Expertin bzw. dem -Experten oder vom Fachunternehmen in der „Bestätigung zum Antrag“ für die Antragsstellung sowie in der „Bestätigung nach Durchführung“ im Rahmen des Verwendungsnachweises anzugeben. Der Zeitpunkt des Inkrafttretens sowie die [Version] einer Fassung sind jeweils in folgender Tabelle vermerkt:\n",
      "\n",
      "{'placeholder_type': 'Institution', 'text': 'KfW'}\n",
      "{'placeholder_type': 'Version', 'text': 'Versionsnummer'}\n",
      "\n",
      "\n",
      "Die Tabelle zeigt Änderungen und Notizen zu verschiedenen Versionen von Richtlinien, die zu bestimmten Daten in Kraft treten. Hier sind die Details:\n",
      "\n",
      "1. **[Version]**:\n",
      "   - **Datum des Inkrafttretens**: [Datum]\n",
      "   - **Änderung/Notiz**: Anpassungen an die neue Förderrichtlinie BEG EM, einschließlich Maßnahmen zur Emissionsminderung von Biomasseheizungen, Klarstellungen zu Wärmepumpen-Hybridheizungskompaktgeräten, Ergänzungen zu wasserstofffähigen Heizungen, Gebäude- und Wärmenetzen, Anforderungen an den Klimageschwindigkeits-Bonus, Erläuterungen zu den Höchstgrenzen der förderfähigen Ausgaben, Verschiebung des Abschnitts Umfeldmaßnahmen, und weitere redaktionelle Änderungen.\n",
      "\n",
      "2. **[Version]**:\n",
      "   - **Datum des Inkrafttretens**: [Datum]\n",
      "   - **Änderung/Notiz**: Streichung Neubau, Zuordnung der Kosten bei Einbau mehrerer Wärmeerzeuger, Anpassung der Definition von grünem Wasserstoff, Definition förderfähiger Kosten bei PVT-Kollektoren, Konkretisierung bzgl. Wärmepumpen als Teil einer Lüftungsanlage, und weitere redaktionelle Anpassungen.\n",
      "\n",
      "3. **[Version]**:\n",
      "   - **Datum des Inkrafttretens**: [Datum]\n",
      "   - **Änderung/Notiz**: Anpassungen an neue Förderrichtlinien.\n",
      "\n",
      "{'placeholder_type': 'Version', 'text': 'Version 9.0'}\n",
      "{'placeholder_type': 'Datum', 'text': '01.01.2024'}\n",
      "{'placeholder_type': 'Version', 'text': 'Version 8.0'}\n",
      "{'placeholder_type': 'Datum', 'text': '20.06.2023'}\n",
      "{'placeholder_type': 'Version', 'text': 'Version 7.0'}\n",
      "{'placeholder_type': 'Datum', 'text': '01.01.2023'}\n",
      "\n",
      "\n",
      "Die Tabelle zeigt verschiedene Versionen eines Dokuments mit ihren jeweiligen Veröffentlichungsdaten und den wichtigsten Änderungen oder Ergänzungen.\n",
      "\n",
      "1. [Version]: Veröffentlicht am [Datum], beinhaltet die Definition von Worst-Performing-Buildings und weitere redaktionelle Anpassungen.\n",
      "2. [Version]: Veröffentlicht am [Datum], hebt förderfähige Kosten für gasbetriebene Anlagen und ertragsabhängige Solarthermie auf, konkretisiert den Heizungs-Tausch-Bonus und enthält weitere redaktionelle Anpassungen.\n",
      "3. [Version]: Veröffentlicht am [Datum], ergänzt nicht förderfähige Kosten im Neubau (Wärmeerzeuger auf Basis des Energieträgers Gas) in Nummer 9.2.\n",
      "4. [Version]: Veröffentlicht am [Datum], verschiebt nicht förderfähige Kosten in Nummer 9, konkretisiert den sommerlichen Wärmeschutz (Nummer 2.5) und begrenzt zeitlich die Leistungen für Inspektion, Wartung und Garantieverlängerungen.\n",
      "5. [Version]: Veröffentlicht am [Datum], ergänzt die NH-Klasse und enthält weitere Klarstellungen/Ergänzungen.\n",
      "6. [Version]: Veröffentlicht am [Datum], ergänzt BEG WG/NWG.\n",
      "7. [Version]: Veröffentlicht am [Datum], ist eine Vorläuferversion und gilt nur für BEG EM.\n",
      "Auf den Programmseiten [BAFA] bzw. den Produktseiten [KfW] zur BEG finden Sie die jeweils aktuelle Version des Infoblatts. Die Speicherung der für einen Antrag jeweils maßgeblichen Fassung des Infoblatts wird Antragsstellenden daher empfohlen. Vorangegangene Versionen sind im [KfW]-Downloadcenter Inlandsförderung sowie im [KfW]-Partnerportal verfügbar ([URL])([URL] bzw. ([URL])[URL]).\n",
      "\n",
      "{'placeholder_type': 'Version', 'text': 'Version 6.0'}\n",
      "{'placeholder_type': 'Datum', 'text': '22.09.2022'}\n",
      "{'placeholder_type': 'Version', 'text': 'Version 5.0'}\n",
      "{'placeholder_type': 'Datum', 'text': '15.08.2022'}\n",
      "{'placeholder_type': 'Version', 'text': 'Version 4.0'}\n",
      "{'placeholder_type': 'Datum', 'text': '20.04.2022'}\n",
      "{'placeholder_type': 'Version', 'text': 'Version 3.0'}\n",
      "{'placeholder_type': 'Datum', 'text': '01.02.2022'}\n",
      "{'placeholder_type': 'Version', 'text': 'Version 2.0'}\n",
      "{'placeholder_type': 'Datum', 'text': '21.10.2021'}\n",
      "{'placeholder_type': 'Version', 'text': 'Version 1.0'}\n",
      "{'placeholder_type': 'Datum', 'text': '01.05.2021'}\n",
      "{'placeholder_type': 'Version', 'text': 'Version 0.0'}\n",
      "{'placeholder_type': 'Datum', 'text': '01.01.2021'}\n",
      "{'placeholder_type': 'BAFA', 'text': 'BAFA'}\n",
      "{'placeholder_type': 'KfW', 'text': 'KfW'}\n",
      "{'placeholder_type': 'KfW', 'text': 'KfW'}\n",
      "{'placeholder_type': 'KfW', 'text': 'KfW'}\n",
      "{'placeholder_type': 'URL', 'text': '<http://www.kfw.de/archiv-4863>'}\n",
      "{'placeholder_type': 'URL', 'text': 'www.kfw.de/archiv-4863'}\n",
      "{'placeholder_type': 'URL', 'text': '<http://www.kfw.de/partnerportal>'}\n",
      "{'placeholder_type': 'URL', 'text': 'www.kfw.de/partnerportal'}\n",
      "\n",
      "\n",
      "Inhalt 1 Förderfähige Maßnahmen und Leistungen – Vorbemerkungen 6 1.1 Grundsätzliches zu den Fördermaßnahmen und zum Förderumfang 7 1.2 Grundsätzliches zur Prüfung der förderfähigen Maßnahmen und der Rechnungsprüfung 8 1.3 Gemischt genutzte Gebäude 9 1.3.1 Gemischt genutzte Wohngebäude - Förderung der Gebäudeteile mit Nichtwohnnutzung 9 1.3.2 Gemischt genutzte Nichtwohngebäude - Förderung der Gebäudeteile mit Wohnnutzung 10 1.3.3 Förderung spezifischer Wohn- bzw. Nichtwohngebäudemaßnahmen der BEG EM in gemischt genutzten Gebäuden 10 1.4 Erweiterung/Anbau/Ausbau/Umwidmung 11 1.5 Eigenleistungen 12 1.5.1 Eigenleistungen von Privatpersonen 12 1.5.2 Eigenleistungen von Unternehmen 12 1.6 Worst-Performing-Buildings (WPB) im Sinne der BEG 13 1.7 Serielles Sanieren im Sinne der BEG WG 13 2 Maßnahmen an der Gebäudehülle 14 2.1 Außenwände 14 2.2 Dachflächen 15 2.3 Decken und Wände gegen unbeheizte Räume, Bodenflächen 15 2.4 Fenster, Fenstertüren, Dachflächenfenster, Glasdächer, Außentüren, Vorhangfassaden und Tore 16 2.5 Sommerlicher Wärmeschutz 17 3 Anlagentechnik (außer Heizung) 17 3.1 Wohngebäude sowie Nichtwohngebäude: Einbau, Austausch oder Optimierung raumluft- und klimatechnischer Anlagen inklusive Wärme-/Kälterückgewinnung 17 3.2 Wohngebäude: Erstinstallation/Erneuerung von Lüftungsanlagen 18 3.3 Nichtwohngebäude: Erstinstallation/Erneuerung von Lüftungsanlagen 18 3.4 Nichtwohngebäude: Austausch von Komponenten in bestehenden Lüftungsanlagen 18 3.5 Wohngebäude („Efficiency Smart Home“): Einbau digitaler Systeme zur energetischen Betriebs- und Verbrauchsoptimierung bzw. zur Verbesserung der Netzdienlichkeit der technischen Anlagen des Gebäudes oder des angeschlossenen Gebäudenetzes 18 3.5.1 Smart-Meter, Mess-, Steuerungs- und Regeltechnik 18 3.5.2 Systemtechnik 19 3.5.3 Schalttechnik, Tür- und Antriebssysteme 19 3.5.4 notwendige Elektroarbeiten 19 3.5.5 Energiemanagementsysteme, Einregulierung 19 3.6 Nichtwohngebäude: Einbau von Mess-, Steuer- und Regelungstechnik 19 3.7 Nichtwohngebäude: Kältetechnik zur Raumkühlung 20 3.8 Nichtwohngebäude: Energieeffiziente Beleuchtungssysteme 20 4 Anlagen zur Wärmeerzeugung (Heizungstechnik) 20 4.1 Anlagen zur Wärmeerzeugung 21 4.1.1 Solarthermische Anlagen 21 4.1.2 Biomasseheizungen 21\n",
      "\n",
      "\n",
      "\n",
      "4.1.3 Elektrisch angetriebene Wärmepumpen 4.1.4 Brennstoffzellenheizung 4.1.5 Wasserstofffähige Heizungen 4.1.6 Innovative Heizungstechnik auf Basis erneuerbarer Energien 4.1.7 Errichtung, Umbau oder Erweiterung eines Gebäudenetzes 4.1.8 Anschluss an ein Gebäude- oder Wärmenetz 4.2 Weitere förderfähige Maßnahmen 4.2.1 Provisorische Heiztechnik bei Heizungsdefekt 4.2.2 Mess-, Steuer- und Regelungstechnik (MSR), Gebäudeautomation, Energiemanagementsysteme 4.2.3 Maßnahmen zur Visualisierung des Ertrags Erneuerbarer Energien: 4.2.4 Wärmespeicher 4.2.5 Warmwasserbereitung 4.2.6 Wärmeverteilung und Wärmeübergabe 4.2.7 Heiz-, Technik- und Speicherraum 4.2.8 Abgassysteme und Schornsteine 4.2.9 Demontagearbeiten 4.3 Voraussetzungen für den Klimageschwindigkeits-Bonus 5 Heizungsoptimierung 5.1 Verbesserung der Anlageneffizienz 5.2 Emissionsminderung von Biomasseheizungen 6 Fachplanung und Baubegleitung 6.1 Energetische Fachplanung und Baubegleitung 6.1.1 Konzeptionierung und Bestandsaufnahme 6.1.2 Planung und Nachweisführung 6.1.3 Beratungsleistungen 6.1.4 In Vorbereitung der Baubegleitung 6.1.5 Während der Baubegleitung 6.1.6 Nach der Umsetzung der Maßnahme 6.2 Leistungen zur Dokumentation der energetischen Fachplanung und Baubegleitung 6.3 Beratungs- und Planungsleistungen zur Nachhaltigkeit und Nachhaltigkeitszertifizierung 6.3.1 Projektvorbereitung 6.3.2 Vorplanung, Entwurfsplanung und Ausführungsplanung 6.3.3 Vorbereitung der Vergabe 6.3.4 Projektdokumentation 6.3.5 Besondere planungsbegleitende Leistungen 7 Anlagen zur Stromerzeugung 8 Umfeldmaßnahmen 9 Nicht förderfähige Maßnahmen 9.1 Gebäudehülle (nicht förderfähige Kosten) 9.2 Wärmeerzeuger/Heizungsanlagen (nicht förderfähige Kosten) 9.3 Anlagen zur Stromerzeugung (nicht förderfähige Kosten) 9.4 Anlagentechnik außer Heizung (nicht förderfähige Kosten)\n",
      "\n",
      "\n",
      "\n",
      "9.5 Sanitäreinrichtungen (Abgrenzung der förderfähigen zu den nicht förderfähigen Kosten) [Seitenzahl] 9.6 Computertechnik und dazugehörige Peripherie (nicht förderfähige Kosten) [Seitenzahl] 9.7 Sonstige Arbeiten und Leistungen - nicht förderfähige Kosten [Seitenzahl] Impressum [Seitenzahl]\n",
      "\n",
      "{'placeholder_type': 'Seitenzahl', 'text': '37'}\n",
      "{'placeholder_type': 'Seitenzahl', 'text': '37'}\n",
      "{'placeholder_type': 'Seitenzahl', 'text': '37'}\n",
      "{'placeholder_type': 'Seitenzahl', 'text': '38'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for answer in chunks_processed:\n",
    "    clean_text = answer['clean_text']\n",
    "    print(f'{clean_text}\\n')\n",
    "\n",
    "    for ph in answer['placeholders']:\n",
    "        print(ph)\n",
    "    print('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_pages_clean(sql_con,table,table_clean_documents):\n",
    "\n",
    "    records = sql_con.get_all_records_as_dict(table)\n",
    "    clean_records = sql_con.get_all_records_as_dict(table_clean_documents)\n",
    "    processed_list = {entry['id']: entry for entry in clean_records}\n",
    "\n",
    "    docs = []\n",
    "    sub_docs = []\n",
    "    act_doc = None\n",
    "    for n,i in enumerate(records):\n",
    "        id = i['id']\n",
    "        pdf_name = i['pdf_name']\n",
    "        if id in processed_list:\n",
    "            vals = processed_list.get(id)\n",
    "            i.update(vals)\n",
    "        \n",
    "        if act_doc != pdf_name and n > 0:\n",
    "            act_doc = pdf_name\n",
    "            docs.append(sub_docs)\n",
    "            sub_docs = []\n",
    "            sub_docs.append(i)\n",
    "        elif n == len(records)-1:\n",
    "            sub_docs.append(i)\n",
    "            docs.append(sub_docs)\n",
    "        else:\n",
    "            act_doc = pdf_name\n",
    "            sub_docs.append(i)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from chunking_embeding_docs import load_documents_pages,config as db_config,db_name,merge_pages,process_metadata_csv\n",
    "from database_manager import MySQLDB\n",
    "\n",
    "\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "dir_main = '/Users/rodolfocacacho/Documents/Documents/MAI/Master Thesis/Code/rag_clean_v2'\n",
    "os.chdir(dir_main)\n",
    "\n",
    "metadata_csv = 'data/documents/metadata/Files_date_version.csv'\n",
    "df_codes = process_metadata_csv(metadata_csv)\n",
    "\n",
    "sql_embed_table = 'embedding_table_pinecone_sparse_new'\n",
    "new_docs = []\n",
    "\n",
    "table_pages_clean = 'table_documents_clean'\n",
    "table_documents_name = 'table_documents'\n",
    "\n",
    "sql_con = MySQLDB(config=db_config,database_name=db_name)\n",
    "\n",
    "docs = load_documents_pages_clean(sql_con=sql_con,\n",
    "                                  table=table_documents_name,\n",
    "                                  table_clean_documents=table_pages_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5 Brennstoffaustragung, undförderung und undzufuhr (Biomasseanlagen)..................................................20\n",
      "... (rest of the text)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Original text string\n",
    "text = \"\"\"4.5 Brennstoffaustragung, -förderung und -zufuhr (Biomasseanlagen)..................................................20\n",
    "... (rest of the text)\"\"\"\n",
    "\n",
    "# Function to merge entries with hyphens based on the prefix\n",
    "def merge_entries(text):\n",
    "    # Regular expression to find patterns with a prefix and hyphenated words\n",
    "    pattern = re.compile(r'(\\b\\w+)([a-zA-Z]*)\\s*-\\w+')\n",
    "    \n",
    "    # Find all matches with a prefix followed by hyphenated terms\n",
    "    matches = pattern.findall(text)\n",
    "    \n",
    "    # Replace hyphenated terms with full words\n",
    "    for match in matches:\n",
    "        prefix = match[0] + match[1]\n",
    "        text = re.sub(r'(?<!\\w)-(\\w+)', prefix + r'\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the function\n",
    "merged_text = merge_entries(text)\n",
    "print(merged_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_unstructured",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
